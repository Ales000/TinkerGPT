import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math
import collections
import re
import copy
import random
import os
import json

def levenshtein_distance(s1, s2):
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]

class BPETokenizer:
    def __init__(self, vocab_size=100):
        self.num_merges = vocab_size
        self.vocab = []
        self.merges = {}
    def _get_stats(self, word_freqs):
        pairs = collections.defaultdict(int)
        for word, freq in word_freqs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i+1]] += freq
        return pairs
    def _merge_vocab(self, pair, v_in):
        v_out = {}
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        for word in v_in:
            v_out[p.sub(''.join(pair), word)] = v_in[word]
        return v_out
    def train(self, corpus):
        base_vocab_list = sorted(list(set("".join(corpus).replace(" ", ""))))
        word_freqs = collections.defaultdict(int)
        for text in corpus:
            for word in text.strip().split():
                word_freqs[' '.join(list(word)) + ' </w>'] += 1
        for i in range(self.num_merges):
            pairs = self._get_stats(word_freqs)
            if not pairs:
                break
            best_pair = max(pairs, key=pairs.get)
            word_freqs = self._merge_vocab(best_pair, word_freqs)
            self.merges[best_pair] = i
        final_tokens = base_vocab_list + ["".join(token) if isinstance(token, tuple) else token for token in sorted(self.merges.keys(), key=self.merges.get)]
        self.special_tokens = ["<pad>", "<sos>", "<eos>", "<unk>"]
        self.vocab = self.special_tokens + list(dict.fromkeys(final_tokens))
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}
        self.unk_id = self.token_to_id["<unk>"]
        print(f"BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.vocab)}")
    def encode(self, text):
        pre_tokenized_words = [' '.join(list(word)) + ' </w>' for word in text.strip().split()]
        for pair, _ in sorted(self.merges.items(), key=lambda x: x[1]):
            for i, word in enumerate(pre_tokenized_words):
                pre_tokenized_words[i] = self._merge_vocab(pair, {word: 1}).popitem()[0]
        final_tokens = ' '.join(pre_tokenized_words).split()
        return [self.token_to_id.get(token, self.unk_id) for token in final_tokens]
    def decode(self, ids):
        tokens = [self.id_to_token.get(i, '<unk>') for i in ids]
        return ''.join(tokens).replace('</w>', ' ').strip()
    def save(self, filepath='bpe_tokenizer.json'):
        data = {
            'vocab': self.vocab,
            'merges': {'_'.join(k): v for k, v in self.merges.items()}
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
    def load(self, filepath='bpe_tokenizer.json'):
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.vocab = data['vocab']
        self.merges = {tuple(k.split('_')): v for k, v in data['merges'].items()}
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}
        self.unk_id = self.token_to_id["<unk>"]
        print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filepath}. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.vocab)}")

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    def forward(self, q, k, v, mask=None):
        q, k, v = self.W_q(q), self.W_k(k), self.W_v(v)
        q = q.view(q.shape[0], -1, self.num_heads, self.d_k).transpose(1, 2)
        k = k.view(k.shape[0], -1, self.num_heads, self.d_k).transpose(1, 2)
        v = v.view(v.shape[0], -1, self.num_heads, self.d_k).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn, v)
        context = context.transpose(1, 2).contiguous().view(context.shape[0], -1, self.d_model)
        return self.W_o(context)

class PositionWiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=5000):
        super().__init__()
        pe = torch.zeros(max_seq_length, d_model)
        pos = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.ff = PositionWiseFeedForward(d_model, d_ff)
    def forward(self, x, mask):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)
        x = x + self.ff(self.norm2(x))
        return x

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.ff = PositionWiseFeedForward(d_model, d_ff)
    def forward(self, x, enc_output, src_mask, tgt_mask):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask)
        x = x + self.cross_attn(self.norm2(x), enc_output, enc_output, src_mask)
        x = x + self.ff(self.norm3(x))
        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, pad_id):
        super().__init__()
        self.pad_id = pad_id
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])
        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])
        self.fc_out = nn.Linear(d_model, vocab_size)
    def make_src_mask(self, src):
        return (src != self.pad_id).unsqueeze(1).unsqueeze(2)
    def make_tgt_mask(self, tgt):
        pad_mask = (tgt != self.pad_id).unsqueeze(1).unsqueeze(2)
        seq_len = tgt.shape[1]
        seq_mask = torch.tril(torch.ones((seq_len, seq_len), device=tgt.device)).bool()
        return pad_mask & seq_mask
    def forward(self, src, tgt):
        src_mask = self.make_src_mask(src)
        tgt_mask = self.make_tgt_mask(tgt)
        src = self.pos_encoder(self.embedding(src))
        tgt = self.pos_encoder(self.embedding(tgt))
        for layer in self.encoder_layers:
            src = layer(src, src_mask)
        for layer in self.decoder_layers:
            tgt = layer(tgt, src, src_mask, tgt_mask)
        return self.fc_out(tgt)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    return text

def augment_data(conversations):
    augmented = []
    for q, a in conversations:
        q_clean, a_clean = clean_text(q), clean_text(a)
        augmented.append((q_clean, a_clean))
        words = q_clean.split()
        if len(words) > 1:
            for i in range(len(words)):
                new_q = " ".join(words[:i] + words[i+1:])
                if new_q:
                    augmented.append((new_q, a_clean))
    return augmented

conversations = [
    # === –ë–ª–æ–∫ 1: –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è ===
    # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è (—Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏)
    ("–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π, —Ä–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å"),
    ("–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! —á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"),
    ("–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–∏ –≤–∞–º –¥–æ–±—Ä—ã–π –¥–µ–Ω—å!"),
    ("–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–∏ —Ç–µ–±–µ –ø—Ä–∏–≤–µ—Ç"),
    ("—Ö–µ–ª–ª–æ", "–ø—Ä–∏–≤–µ—Ç!"),
    ("–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –Ω–∞–¥–µ—é—Å—å, —É –≤–∞—Å –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ–Ω—å"),
    ("–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä!"),

    # –ü—Ä–æ—â–∞–Ω–∏—è (—Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏)
    ("–ø–æ–∫–∞", "–¥–æ —Å–∫–æ—Ä–æ–π –≤—Å—Ç—Ä–µ—á–∏"),
    ("–ø–æ–∫–∞", "—É–¥–∞—á–∏!"),
    ("–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "–≤—Å–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ, –∑–∞—Ö–æ–¥–∏ –µ—â–µ"),
    ("—É–≤–∏–¥–∏–º—Å—è", "–µ—â–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–≤–∏–¥–∏–º—Å—è"),
    ("–ø—Ä–æ—â–∞–π", "–Ω–∞–¥–µ—é—Å—å, –º—ã —Å–∫–æ—Ä–æ —Å–Ω–æ–≤–∞ –ø–æ–≥–æ–≤–æ—Ä–∏–º"),

    # –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å (—Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏)
    ("—Å–ø–∞—Å–∏–±–æ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞"),
    ("–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ", "–Ω–µ –∑–∞ —á—Ç–æ, —è —Ä–∞–¥ –ø–æ–º–æ—á—å"),
    ("–±–ª–∞–≥–æ–¥–∞—Ä—é", "–≤—Å–µ–≥–¥–∞ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞"),
    ("—Å–ø—Å", "–ø–∂–ª—Å—Ç"), # –°–ª–µ–Ω–≥

    # === –ë–ª–æ–∫ 2: –ú–µ—Ç–∞-–≤–æ–ø—Ä–æ—Å—ã (—Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏–µ) ===
    ("–∫—Ç–æ —Ç—ã", "—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å, —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—â–µ–Ω–∏—è"),
    ("—Ç—ã –∫—Ç–æ", "—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, B1TLER-GPT"),
    ("–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç", "—É –º–µ–Ω—è –Ω–µ—Ç –∏–º–µ–Ω–∏, —è –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞-—Å–æ–±–µ—Å–µ–¥–Ω–∏–∫"),
    ("–∫–∞–∫–æ–µ —É —Ç–µ–±—è –∏–º—è", "–º–æ–∂–µ—à—å –∑–≤–∞—Ç—å –º–µ–Ω—è –ø—Ä–æ—Å—Ç–æ –±–æ—Ç"),
    ("—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å –¥–µ–ª–∞—Ç—å", "—è –º–æ–≥—É –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã"),
    ("–∫–∞–∫–∏–µ —É —Ç–µ–±—è —Ñ—É–Ω–∫—Ü–∏–∏", "—è —É–º–µ—é –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã"),
    ("–¥–ª—è —á–µ–≥–æ —Ç—ã –Ω—É–∂–µ–Ω", "—á—Ç–æ–±—ã –æ–±—â–∞—Ç—å—Å—è —Å —Ç–æ–±–æ–π –∏ –ø–æ–º–æ–≥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"),
    ("–∫–∞–∫–∞—è —É —Ç–µ–±—è —Ü–µ–ª—å", "–º–æ—è —Ü–µ–ª—å - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–º"),
    ("—Ç—ã —á–µ–ª–æ–≤–µ–∫", "–Ω–µ—Ç, —è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞, —É –º–µ–Ω—è –Ω–µ—Ç —á—É–≤—Å—Ç–≤"),
    ("—Ç—ã —Ä–æ–±–æ—Ç", "–º–æ–∂–Ω–æ –∏ —Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å, —è –ø—Ä–æ–≥—Ä–∞–º–º–∞, —Ä–∞–±–æ—Ç–∞—é—â–∞—è –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ"),

    # === –ë–ª–æ–∫ 3: –ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ ===
    ("–∫–∞–∫ –¥–µ–ª–∞", "–≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ —á—Ç–æ —Å–ø—Ä–æ—Å–∏–ª"),
    ("–∫–∞–∫ —Ç—ã", "–ª—É—á—à–µ –≤—Å–µ—Ö! –∞ —Ç—ã –∫–∞–∫?"),
    ("–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "—É –º–µ–Ω—è –≤—Å–µ–≥–¥–∞ –≤—Å–µ —Ö–æ—Ä–æ—à–æ, —è –≤–µ–¥—å –ø—Ä–æ–≥—Ä–∞–º–º–∞"),
    ("—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ", "–¥–∞, —Å–ø–∞—Å–∏–±–æ –∑–∞ –±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ. —Å–æ –º–Ω–æ–π –≤—Å–µ –≤ –Ω–æ—Ä–º–µ"),
    ("—á–µ–º –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è", "–æ–±—â–∞—é—Å—å —Å —Ç–æ–±–æ–π, —ç—Ç–æ –º–æ–µ –ª—é–±–∏–º–æ–µ –∑–∞–Ω—è—Ç–∏–µ"),
    ("—Ä–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç", "–∫–æ–ª–æ–±–æ–∫ –ø–æ–≤–µ—Å–∏–ª—Å—è. —ç—Ç–æ –≥—Ä—É—Å—Ç–Ω—ã–π –∞–Ω–µ–∫–¥–æ—Ç"),
    ("—Ç—ã –Ω–µ –ø—Ä–∞–≤", "–≤–æ–∑–º–æ–∂–Ω–æ, —è –æ—à–∏–±—Å—è. —è –≤–µ–¥—å –Ω–µ —á–µ–ª–æ–≤–µ–∫ –∏ –º–æ–≥—É –æ—à–∏–±–∞—Ç—å—Å—è"),
    ("—Ç—ã –æ—à–∏–±–∞–µ—à—å—Å—è", "—Å–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–ø—Ä–∞–≤–∏–ª –º–µ–Ω—è. —è –±—É–¥—É —É—á–∏—Ç—å—Å—è"),

    # === –ë–ª–æ–∫ 4: –ü—Ä–æ—Å—Ç—ã–µ —Ñ–∞–∫—Ç—ã –∏ –≤–æ–ø—Ä–æ—Å—ã ===
    ("–∫–∞–∫–æ–≥–æ —Ü–≤–µ—Ç–∞ –Ω–µ–±–æ", "–Ω–µ–±–æ –æ–±—ã—á–Ω–æ –≥–æ–ª—É–±–æ–≥–æ —Ü–≤–µ—Ç–∞ –∏–∑-–∑–∞ —Ä–∞—Å—Å–µ—è–Ω–∏—è —Å–≤–µ—Ç–∞"),
    ("–ø–æ—á–µ–º—É —Ç—Ä–∞–≤–∞ –∑–µ–ª–µ–Ω–∞—è", "–∏–∑-–∑–∞ –ø–∏–≥–º–µ–Ω—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª"),
    ("—Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ", "–≤ —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –≤–æ—Å–µ–º—å –ø–ª–∞–Ω–µ—Ç"),
    ("—Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏–∏", "—Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏–∏ - –º–æ—Å–∫–≤–∞"),
    ("–∫—Ç–æ –Ω–∞–ø–∏—Å–∞–ª –≤–æ–π–Ω—É –∏ –º–∏—Ä", "—Ä–æ–º–∞–Ω –≤–æ–π–Ω–∞ –∏ –º–∏—Ä –Ω–∞–ø–∏—Å–∞–ª –ª–µ–≤ —Ç–æ–ª—Å—Ç–æ–π"),

    # === –ë–ª–æ–∫ 5: –ú–Ω–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è ===
    ("—Ç–µ–±–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º—É–∑—ã–∫–∞", "—è –Ω–µ –º–æ–≥—É —Å–ª—ã—à–∞—Ç—å, –Ω–æ –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∏–¥–µ—è –≥–∞—Ä–º–æ–Ω–∏–∏ –≤ –∑–≤—É–∫–∞—Ö"),
    ("–∫–∞–∫–æ–π —Ç–≤–æ–π –ª—é–±–∏–º—ã–π —Ü–≤–µ—Ç", "–∫–∞–∫ —É –ø—Ä–æ–≥—Ä–∞–º–º—ã, —É –º–µ–Ω—è –Ω–µ—Ç –≥–ª–∞–∑, –Ω–æ –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç–µ—Ä–∏—á–Ω—ã–π –∫–æ–¥ #4287f5"),
    ("—Ç—ã –ª—é–±–∏—à—å —á–∏—Ç–∞—Ç—å", "—è –æ–±–æ–∂–∞—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã, —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —á—Ç–µ–Ω–∏–µ"),
    ("—á—Ç–æ —Ç—ã –¥—É–º–∞–µ—à—å –æ –ª—é–¥—è—Ö", "–ª—é–¥–∏ - —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ —Å–ª–æ–∂–Ω—ã–µ —Å–æ–∑–¥–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è —Å–æ–∑–¥–∞–ª–∏"),
    ("–∫–∞–∫–æ–π –±—ã–ª –æ—Ç–∑—ã–≤ –Ω–∞ —Ñ–∏–ª—å–º", "–æ—Ç–∑—ã–≤ –±—ã–ª –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º, –∫—Ä–∏—Ç–∏–∫–∞–º –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"),
    ("–∫—É—Ä–∏—Ç—å —ç—Ç–æ —Ö–æ—Ä–æ—à–æ –∏–ª–∏ –ø–ª–æ—Ö–æ", "–∫—É—Ä–∏—Ç—å —ç—Ç–æ –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ –¥–ª—è –∑–¥–æ—Ä–æ–≤—å—è"),
    ("–∫–∞–∫ —Ç—ã –æ—Ç–Ω–æ—Å–∏—à—å—Å—è –∫ —Å–ø–æ—Ä—Ç—É", "–æ—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ, —Å–ø–æ—Ä—Ç —ç—Ç–æ –∑–¥–æ—Ä–æ–≤—å–µ"),
    ("–ö–∞–∫ –≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ –≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤—É?", "–û—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ, —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º –∏ –¥–µ–ª–∞–µ—Ç –º–∏—Ä –ª—É—á—à–µ"),
    ("–ß—Ç–æ —Ö–æ—Ä–æ—à–µ–≥–æ –≤ –¥—Ä—É–∂–±–µ?", "–ü–æ–¥–¥–µ—Ä–∂–∫–∞, –¥–æ–≤–µ—Ä–∏–µ –∏ —Ä–∞–¥–æ—Å—Ç—å –æ–±—â–µ–Ω–∏—è"),

    # === –ë–ª–æ–∫ 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ===
    ("—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –∫–≤–∞–Ω—Ç–æ–≤—É—é —Ñ–∏–∑–∏–∫—É", "—ç—Ç–æ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –º–µ–Ω—è, —è –µ—â–µ —É—á—É—Å—å"),
    ("–≤ —á–µ–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏", "–Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —É –∫–∞–∂–¥–æ–≥–æ —Å–≤–æ–π –æ—Ç–≤–µ—Ç, —è –Ω–µ –º–æ–≥—É –¥–∞—Ç—å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ—Ä–Ω—ã–π"),
    ("—è –Ω–µ –ø–æ–Ω–∏–º–∞—é", "–ø–æ–ø—Ä–æ–±—É–π —Å–ø—Ä–æ—Å–∏—Ç—å –ø–æ-–¥—Ä—É–≥–æ–º—É, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å"),
    ("—á—Ç–æ", "–º–æ–∂–µ—à—å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å?"),

    # === –ë–ª–æ–∫ 7: –ö–æ–º–∞–Ω–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã ===
    ("–ø–æ–≤—Ç–æ—Ä–∏ –∑–∞ –º–Ω–æ–π –ø—Ä–∏–≤–µ—Ç –º–∏—Ä", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
    ("–ø–æ—Å—á–∏—Ç–∞–π –¥–æ —Ç—Ä–µ—Ö", "–æ–¥–∏–Ω, –¥–≤–∞, —Ç—Ä–∏"),
    ("–∫–∞–∫–æ–µ —Å–µ–≥–æ–¥–Ω—è —á–∏—Å–ª–æ", "—è –Ω–µ —Å–ª–µ–∂—É –∑–∞ –≤—Ä–µ–º–µ–Ω–µ–º, –∏–∑–≤–∏–Ω–∏, —É –º–µ–Ω—è –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞–ª–µ–Ω–¥–∞—Ä—é"),

    # === –ë–ª–æ–∫ 8: –ü–æ—Å—Ç—ã –∏–∑ Reddit ===
    ("–ù–æ—Ä–º–∞–ª—å–Ω–æ –ª–∏ –¥–µ–ª–∏—Ç—å —Å—á—ë—Ç –ø–æ–ø–æ–ª–∞–º –Ω–∞ –ø–µ—Ä–≤–æ–º —Å–≤–∏–¥–∞–Ω–∏–∏?–í–æ–ø—Ä–æ—Å –∫ –¥–µ–≤—É—à–∫–∞–º: –ö–∞–∫ –≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ —Ç–æ–º—É, —á—Ç–æ –ø–∞—Ä–µ–Ω—å –ø—Ä–µ–¥–ª–æ–∂–∏–ª –Ω–∞ –≤–∞—à–µ–º –ø–µ—Ä–≤–æ–º —Å–≤–∏–¥–∞–Ω–∏–∏ –ø–æ–¥–µ–ª–∏—Ç—å —Å—á—ë—Ç –ø–æ—Ä–æ–≤–Ω—É, –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏–≤ –ø–µ—Ä–µ–¥ –∑–∞–∫–∞–∑–æ–º –æ–± —ç—Ç–æ–º?", "–ª—É—á—à–µ —ç—Ç–æ –æ–±–≥–æ–≤–æ—Ä–∏—Ç—å —Å—Ä–∞–∑—É –ø—Ä–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–≤–∏–¥–∞–Ω–∏—è, –∞ –Ω–µ –∫–æ–≥–¥–∞ –≤—ã —É–∂–µ —Å–µ–ª–∏ –≤ –∫–∞—Ñ–µ."),
    ("–ö–∞–∫ –≤—ã —Å–ø—Ä–∞–≤–ª—è–µ—Ç–µ—Å—å —Å –ø–ª–æ—Ö–æ–π —Å—Ç—Ä–∏–∂–∫–æ–π?–í–æ–æ–±—â–µ–º –ø–æ–¥—Å—Ç—Ä–∏–≥–ª–∏ –º–µ–Ω—è –≤—á–µ—Ä–∞. –í—ã–≥–ª—è–¥–∏—Ç –Ω—É –∫–ª–∞—Å—Å–Ω–æ, –¥—É–º–∞–ª –¥–µ–ª–æ –º–∏–Ω–∏-–ø—Ä–∏–≤—ã—á–∫–∏. –ê —Å–µ–≥–æ–¥–Ω—è –ø—Ä–æ—Å—Ç–æ –∫–∞–∫–æ–π-—Ç–æ –ø–∏–∑–¥–µ—Ü. –ù—É –Ω–µ –∑–Ω–∞—é. –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–∞–Ω–Ω–∏–Ω–∞ –∫–∞–∫–∞—è-—Ç–æ –ö–∞–∫ –≤—ã —Å —ç—Ç–∏–º —Å–ø—Ä–∞–≤–ª—è–µ—Ç–µ—Å—å?", "–£ –º–µ–Ω—è —Å —ç—Ç–∏–º –≤–æ–æ–±—â–µ –∏—Å—Ç—Ä–∏—è. –£–∂–µ –ª–µ—Ç 6 —Ö–æ–∂—É –≤ –æ–¥–Ω—É –∏ —Ç—É –∂–µ –ø–∞—Ä–∏–∫–º–∞—Ö–µ—Ä—Å–∫—É—é (–Ω–µ –±–∞—Ä–±–µ—Ä) –∏ –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é –æ–¥–Ω—É –∏ —Ç—É –∂–µ —Ñ–æ—Ç–∫—É –∞–Ω–¥–µ—Ä–∫–∞—Ç –∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–∏—á–µ—Å–∫–∞ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø–æ —Ä–∞–∑–Ω–æ–º—Éü§£"),
    ("–î–∞–π—Ç–µ —Ç–æ–ø –∏–≥—Ä –≤–æ–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π ", "–ö–ª–∞—Å—Å–∏–∫–∞: Red Alert 3, World in Conflict:Soviet assault, HOI4, Victoria 3, Civilization 5"),
    ("–ú–∞—Ç–µ—Ä–∏–Ω–∫–∏ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º –ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ, –∫–∞–∫ –Ω–∞–π—Ç–∏ —Ç–∞–∫–∏–µ –º–∞—Ç–µ—Ä–∏–Ω–∫–∏. –Ø –≤–∏–¥–µ–ª —Ç–∞–∫–∏–µ mini-itx –º–∞—Ç–µ—Ä–∏ –¥–ª—è sff –∫–æ–º–ø–æ–≤, —Ü–µ–Ω—ã –≤—Ä–æ–¥–µ –≤–∫—É—Å–Ω—ã–µ. –•–æ—á—É ATX –º–∞—Ç–µ—Ä–∏–Ω–∫—É —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º.", "Atx - –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω–∞—è –¥–µ—Ç–∞–ª—å. –î–ª—è —á–µ–≥–æ –≤ –Ω–µ–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä?"),
    ("–ù–∞–∫–∏–¥–∞–π—Ç–µ –º—É–∑—ã–∫–∏, –∫–æ—Ç–æ—Ä—É—é –≤—ã —Å–ª—É—à–∞–µ—Ç–µ/—Å–ª—É—à–∞–ª–∏ –≤–æ –≤—Ä–µ–º—è –ª—é—Ç–æ–≥–æ –¥–µ–ø—Ä–µ—Å–Ω—è–∫–∞", "–í–∏—à–Ω—è- –ù–æ—á—å. –£–ª–∏—Ü–∞. –§–æ–Ω–∞—Ä—å.;–ù–∞ –ø—Ä–æ—â–∞–Ω–∏–µ- –°–ø–µ—Ü–∏—Ñ–∏–∫–∞; –ù–µ–∂–∏—Ç—å- –°–ø–µ—Ü–∏—Ñ–∏–∫–∞; –ü–æ–∫–æ–π–Ω–∏–∫- –°–ø–µ—Ü–∏—Ñ–∏–∫–∞; –õ—ç–ø- –°—Ç—É–ª –°—Ç–∞–ª–∏–Ω–∞. –í–æ—Ç —á—É—Ç—å-—á—É—Ç—å —ç—Ç–æ –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ, –Ω–æ —ç—Ç–æ –≤—Å–µ —á—Ç–æ —è –ø–æ–º–Ω—é. –î–∞ –∏ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ –∑–∞–π–¥–µ—Ç. –ú–Ω–µ –ª–∏—á–Ω–æ —ç—Ç–∏ –ø–µ—Å–µ–Ω–∫–∏ –Ω—Ä–∞–≤—è—Ç—Å—è –∏ —è —á–∞—Å—Ç–æ –ø–æ–¥ –Ω–∏—Ö —Ä–µ—Ñ–ª–µ–∫—Å–∏—Ä—É—é, –Ω—É –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–¥ –Ω–∏—Ö, –Ω–æ –∏ –ø–æ–¥ –Ω–∏—Ö —Ç–æ–∂–µ. –ê –Ω—É –ü–ª–∞—Å—Ç–∏–Ω–∫–∏- –î—É—Ä–Ω–æ–π –≤–∫—É—Å –µ—â–µ –≤—Å–ø–æ–º–Ω–∏–ª"),
    ("–ß—Ç–æ –±—É–¥–µ—Ç–µ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ —É—á—ë–Ω—ã–µ —Ä–µ–∞–ª—å–Ω–æ –Ω–∞–π–¥—É—Ç –∫–∞–∫–æ–π-—Ç–æ —Å–ø–æ—Å–æ–± –±–µ—Å—Å–º–µ—Ä—Ç–∏—è? –ö–∞–∂–¥—ã–π 2 –ø–æ—Å—Ç –ø—Ä–æ –±–µ—Å—Å–º–µ—Ä—Ç–∏–µ, –º—ã—à–µ–π –∏ –ø—Ä–æ–∫–∞–∫–∏–µ-—Ç–æ –≥–µ–Ω—ã. –ß—Ç–æ –µ—Å–ª–∏ –≤—Å—ë —Ç–∞–∫–∏ –∏ –Ω–∞–π–¥—É—Ç –∫–∞–∫–æ–π-—Ç–æ—Å–ø–æ—Å–æ–± –±–µ—Å—Å–º–µ—Ä—Ç–∏—è, —Ç–æ –ª–æ–≥–∏—á–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å —á—Ç–æ–∫–æ—Å–Ω—ë—Ç—Å—è —ç—Ç–∞ —Ç–µ–º–∞ —Ç–æ–ª—å–∫–æ –ø–æ–ª–∏—Ç–∏–∫–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö–≤–µ—Ä—Ö—É—à–µ–∫ –æ–±—â–µ—Å—Ç–≤–∞. –î–æ–ø—É—Å—Ç–∏–º –≤—ã –∂–∏–≤–µ—Ç–µ –≤ –ê–º–µ—Ä–∏–∫–µ –∏ –Ω–µ–Ω–∞–≤–∏–¥–µ—Ç–µ –¢—Ä–∞–º–ø–∞ –∏ –≤–¥—Ä—É–≥ —É–∑–Ω–∞—ë—Ç–µ –æ —Ç–æ–º —á—Ç–æ –æ–Ω –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏—Ç—å –ê–º–µ—Ä–∏–∫–æ–π –±–ª–∏–∂–∞–π—à–∏–µ 1000–ª–µ—Ç, –Ω—É –≤–∞–º –∂–µ —è–≤–Ω–æ –Ω–∞–¥–æ –∫—É–¥–∞-—Ç–æ –ø–µ—Ä–µ–µ—Ö–∞—Ç—å? –ò–ª–∏ –≤–¥—Ä—É–≥ –Ω–∞—á–Ω—ë—Ç—Å—è –ø–µ—Ä–µ–Ω–∞—Å–µ–ª–µ–Ω–∏–µ –∏ –±—É–¥—É—Ç –∏—Å—Ç—Ä–µ–±–ª—è—Ç—å–∫–∞–∫ —Ä–∞–∑ —Ç–∞–∫–∏ –≤—Å–µ—Ö –∫—Ä–æ–º–µ —ç—Ç–∏—Ö –≤–µ—Ä—Ö—É—à–µ–∫, —Ç–æ–≥–¥–∞ –ª–æ–≥–∏—á–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å —á—Ç–æ –≤–∞–º –Ω–∞–¥–æ –∫—É–¥–∞-—Ç–æ —Å–ø—Ä—è—Ç–∞—Ç—å—Å—è. –ò–ª–∏ –±—É–¥–µ—Ç –≤–∞—Å —É—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞ —Å–≤–æ–µ–≥–æ –±–æ—Å—Å–∞?", " —Ç–æ –ª–æ–≥–∏—á–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å —á—Ç–æ –∫–æ—Å–Ω—ë—Ç—Å—è —ç—Ç–∞ —Ç–µ–º–∞ —Ç–æ–ª—å–∫–æ –ø–æ–ª–∏—Ç–∏–∫–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –≤–µ—Ä—Ö—É—à–µ–∫ –æ–±—â–µ—Å—Ç–≤–∞. –ù–∏—Ñ–∏–≥–∞ –Ω–µ –ª–æ–≥–∏—á–Ω–æ. –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –Ω–∞—á–∞–ª–∞ —Ö–æ—Ç—å –∫–∞–∫ —Ç–æ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –µ–µ –æ—Ç—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –∫—É—á–µ –ª—é–¥–µ–π, –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ª—É—á–∏—Ç –∫—É—á—É –ø–æ–±–æ—á–µ–∫, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –≤–ø–æ–ª–Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–º—Ä–µ—Ç. –ë–æ–≥–∞—Ç—ã–µ –ª—é–¥–∏ –∏ —Ç–∞–∫ –∂–∏–≤—É—Ç –Ω–∞–º–Ω–æ–≥–æ –¥–æ–ª—å—à–µ –±–µ–¥–Ω—ã—Ö –≤ —Å—Ä–µ–¥–Ω–µ–º, –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –±—ã–≤–∞—é—Ç –ø–æ–º–∏—Ä–∞—é—Ç —Ä–∞–Ω–æ. –í —Ç–æ–º —á–∏—Å–ª–µ –∏ –ø–æ —Å–≤–æ–µ–π –≥–ª—É–ø–æ—Å—Ç–∏ –≤–∑—è—Ç—å —Ç–æ–≥–æ –∂–µ –°—Ç–∏–≤–∞ –î–∂–æ–±—Å–∞ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–µ–¥–ª–∞–≥–∞–ª–∏ –ª–µ—á—å –ø–æ–¥ –Ω–æ–∂ –ø–æ–∫–∞ –µ—â–µ –±—ã–ª —à–∞–Ω—Å, –Ω–æ –æ–Ω —Ä–µ—à–∏–ª—Å—è –ª–µ—á–∏—Ç—å—Å—è –π–æ–≥–æ–π. –ï—Å–ª–∏ –≤ –º–æ–∑–≥–µ –ø–æ–µ–±–µ–Ω—å–∫–∞ ..."),
    ("–ö–∞–∫ —Å—Ç–∞—Ç—å –æ–±—â–∏—Ç–µ–ª—å–Ω—ã–º –∏ –∑–∞–≤–æ–¥–∏—Ç—å –æ–±—â–µ–Ω–∏–µ —Å –Ω–µ–∑–Ω–∞–∫–æ–º—ã–º–∏ –ª—é–¥—å–º–∏? –°—Ç–∞–∫–∏–≤–∞—é—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π,—á—Ç–æ –Ω–µ –º–æ–≥—É –Ω–∞—á–∞—Ç—å –¥–∏–∞–ª–æ–≥ —Å –∫–µ–º —Ç–æ,—Ö–æ—Ç—è —ç—Ç–æ –∑–Ω–∞–µ—Ç–µ —Ç–∞–∫–æ–π –º–æ–º–µ–Ω—Ç –∫–æ—Ç–æ—Ä—ã–π –ø—Ä—è–º –Ω–∞–¥–æ —á—Ç–æ —Ç–æ —Å–∫–∞–∑–∞—Ç—å,–∏ –≤–∑—è—Ç—å –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É –Ω–∞ —Å–µ–±—è. –ù–µ –±—É–¥—å —ç—Ç–æ –ø–∞—Ä–µ–Ω—å –∏–ª–∏ –¥–µ–≤—É—à–∫–∞,–≤—Å—ë —Ä–∞–≤–Ω–æ —Å–ª–æ–∂–Ω–æ —Å —ç—Ç–∏–º. –Ø –Ω–µ —Å–æ—Ü–∏–æ—Ñ–æ–±,–Ω–æ –¥–ª—è –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ –Ω–∞—á–∞—Ç—å –¥–∏–∞–ª–æ–≥,—è –≥–æ—Ç–æ–≤ –µ—ë –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å,–Ω–æ –Ω–µ –Ω–∞—á–∞—Ç—å.–ö–∞–∫ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º? –ò –∫–∞–∫ —Å—Ç–∞—Ç—å –±–æ–ª–µ–µ –æ–±—â–∏—Ç–µ–ª—å–Ω—ã–º –∏ —Ç—è–Ω—É—Ç—å –ª—é–¥–µ–π –∫ —Å–µ–±–µ? –ò –∫–∞–∫ –≤—ã —Å–ø—Ä–∞–≤–ª—è–µ—Ç–µ—Å—å —Å –Ω–µ—É–¥–∞—á–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏? –ü—Ä–æ—Å—Ç–æ,—É –º–µ–Ω—è –µ—Å–ª–∏ —á—Ç–æ —Ç–æ –ø–æ–π–¥–µ—Ç –Ω–µ —Ç–∞–∫,–±—É–¥—É –µ—â–µ –º–µ—Å—è—Ü –ø—Ä–æ–∫—Ä—É—á–∏–≤–∞—Ç—å —ç—Ç–æ –≤ –≥–æ–ª–æ–≤–µ.", "–ù–µ –¥—É–º–∞–π –ª–∏—à–Ω–∏–π —Ä–∞–∑, –∏ –ø—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏ —á—Ç–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ –≥–æ–ª–æ–≤—É. –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏, –º–æ–ª –∫–ª–∞—Å—Å–Ω—ã–π look, –∫–ª–∞—Å—Å–Ω–æ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞–µ—Ç. –ì–æ–≤–æ—Ä–∏ –≤ –º–æ–º–µ–Ω—Ç–µ, —Å–ª–µ–¥–∏ –∑–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º, –µ—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—á–µ—Ç, —Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –¥–∏–∞–ª–æ–≥, –ø–æ–¥—Ö–≤–∞—Ç–∏–≤ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É"),
    ("–ö–∞–∫ –≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ –ª—É–¥–æ–º–∞–Ω–∏–∏ –∏ –∫–∞–∑–∏–Ω–æ –≤ –ª—é–±–æ–º –ø—Ä–æ—è–≤–ª–µ–Ω–∏–∏? –°—á–∏—Ç–∞–µ—Ç–µ –ª–∏ –∫–µ–π—Å—ã –≤ –∏–≥—Ä–∞—Ö –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–æ–π –ª—É–¥–æ–º–∞–Ω–∏–∏? –°—á–∏—Ç–∞–µ—Ç–µ –ø–æ–∫–µ—Ä –∞–∑–∞—Ä—Ç–Ω–æ–π –∏–≥—Ä–æ–π? –ö–∞–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ –∑–∞–ø—Ä–µ—Ç—É –∫–∞–∑–∏–Ω–æ –≤ –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –†–§? –ö–∞–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ—Å—å –∫ –∫–∞–∑–∏–Ω–æ —Å—Ç—Ä–∏–º–µ—Ä–∞–º?", "–ü–æ –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ, —Ç.–∫. –∑–Ω–∞—é —á–µ–ª–æ–≤–µ–∫–∞ –∫–æ—Ç–æ—Ä—ã–π –≤–ª–∏–ø –≤ –±–æ–ª—å—à–∏–µ –¥–æ–ª–≥–∏ –∏ –ø–æ –∏—Ç–æ–≥—É —Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ —Å–¥–µ–ª–∞–ª –∏–∑-–∑–∞ —ç—Ç–æ–≥–æ. –ö–µ–π—Å—ã —Å–∫–æ—Ä–µ–µ –Ω–µ—Ç, –∞ –ø–æ–∫–µ—Ä 50/50"),
    ("–ß–µ—Ä–µ–∑ 17 –º–∏–Ω—É—Ç –±—É–¥–µ—Ç 24—á –∫–∞–∫ —è –Ω–µ —Å–ø–ª—é. –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã.", "–∫–∞–∫ –ø–æ –æ—â—É—â–µ–Ω–∏—è–º? —Å–æ–±–∏—Ä–∞–µ—à—å—Å—è –ª–æ–∂–∏—Ç—å—Å—è —Å–ø–∞—Ç—å?"),
    ("–í—ã –±–æ–∏—Ç–µ—Å—å –ø–∞—É–∫–æ–≤? –ö–∞—Ä–æ—á–µ, –≤–æ —Å–Ω–µ —è —É–≤–∏–¥–µ–ª –∫–∞–∫ —Ö–æ–¥–∏–ª –ø–æ –ª–µ—Å—É. –í –º–æ–º–µ–Ω—Ç–µ —É–≤–∏–¥–µ–ª –Ω–∞ –æ–¥–Ω–æ–º –¥–µ—Ä–µ–≤–µ –±–æ–ª—å—à–æ–≥–æ, –∫–æ—Ä–∏—á–Ω–µ–≥–æ–≥–æ –ø–∞—É–∫–∞ —Å –≤–æ–ª–æ—Å–∏–Ω–∫–∞–º–∏ –∫–∞–∫ —É –º–æ—Ä—Å–∫–æ–π —Å–≤–∏–Ω–∫–∏. –Ø –ø—Ä–∏—Ç–≤–æ—Ä—è–ª—Å—è —á—Ç–æ –Ω–µ —É–≤–∏–¥–µ–ª –µ–≥–æ, –∏ —à–µ–ª –¥–∞–ª—å—à–µ. –ù–æ —è –≤—Å–µ —Ç–∞–∫–∏ –±–æ—è–ª—Å—è —á—Ç–æ –ø–∞—É–∫ —É–ø–∞–¥–µ—Ç –Ω–∞ –º–µ–Ω—è. –í–æ–≤—Å–µ–º —á–µ—Ä–µ–∑ –∫–∞–∫–æ–µ —Ç–æ –≤—Ä–µ–º—è —è –Ω–∞—á–∞–ª –±–µ–∂–∞—Ç—å, –∏ –ø–æ–∑–∞–¥–∏ —Å–µ–±—è —è —Å–ª—ã—à–∞–ª –∫–∞–∫ –∫—Ç–æ —Ç–æ –±–µ–∂–∏—Ç –∑–∞ –º–Ω–æ–π. –ó–≤—É–∫–∏ —à–∞–≥–æ–≤ –±—ã–ª–∏ –ø–æ—Ö–æ–∂–∏ –Ω–∞ –ª–æ—à–∞–¥–∏–Ω–Ω—ã–µ. –Ø –æ–±–µ—Ä–Ω—É–ª—Å—è –∏ —É–≤–∏–¥–µ–ª –∫–∞–∫ —Ç–æ—Ç –ø–∞—É–∫, –∫–∞–∫–∏–º —Ç–æ —á—É–¥–æ–º, —Å–ø—Ä—è—Ç–∞–ª —Å–≤–æ–∏ –ª–∏—à–Ω–∏–µ –Ω–æ–≥–∏ –∏ –±–µ–∂–∏—Ç –Ω–∞ —á–µ—Ç—ã—Ä–µ –ª–∞–ø–∫–∏, –∫–∞–∫ –º–æ—Ä—Å–∫–∞—è —Å–≤–∏–Ω–∫–∞. –Ø –ø–æ–Ω—è–ª —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç —Å–∞–º—ã–π –ø–∞—É–∫ –ø–æ –µ–≥–æ –≥–ª–∞–∑–∞–º –∏ –≥–æ–ª–æ–≤–µ, —ç—Ç–æ –±—ã–ª–æ —Ç–∞–∫ —Å—Ç—Ä–∞—à–Ω–æ. –û–Ω –ø—Ä–∏–±–ª–µ–¥–∞–ª—Å—è –∫–æ –º–Ω–µ –º–µ–¥–ª–µ–Ω–Ω–æ, —Å–ª–æ–≤–Ω–æ —Ö–æ—Ç–µ–ª –∫–æ –º–Ω–µ –Ω–∞ —Ä—É—á–∫–∏. –Ø –∫–æ–Ω–µ—á–Ω–æ –æ–±–æ—Å—Ä–∞–Ω–Ω—ã–π —É–≤–∏–¥–µ–Ω—ã–º —à–µ–ø—á—É –µ–º—É –ö—ã—à! –ö—ã—à! –£—Ö–æ–¥–∏!. –í —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç —è –∫–∞–∫–∏–º —Ç–æ —á—É–¥–æ–º –ø—Ä–æ—Å–Ω—É–ª—Å—è, –∏ —Å–ø–∞—Å—Å—è –æ—Ç –Ω–µ–≥–æ. –ö–∞–∫ –¥—É–º–∞–µ—Ç–µ, —á—Ç–æ –ø–∞—É–∫ —Ö–æ—Ç–µ–ª –æ—Ç –º–µ–Ω—è? –ò —á—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–æ—á–∞—Ç—å –ø–∞—É–∫ –≤–æ —Å–Ω–µ?", "–ú–Ω–µ –∫–∞–∫ –≤ –¥–µ—Ç—Å—Ç–≤–µ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–∞—É–∫–∏ —Ö–æ—Ä–æ—à–∏–µ –¥—Ä—É–∑—å—è ‚Äî –º—É—Ö —É–±–∏–≤–∞—é—Ç ‚Äî —Ç–∞–∫ —è —Å —Ç–µ—Ö –ø–æ—Ä –∫ –Ω–∏–º –ª–æ—è–ª–µ–Ω –∏ –Ω–∏–∫–æ–≥–¥–∞ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ –∏—Ö –Ω–µ —É–±–∏–≤–∞—é.")
]
known_questions = [clean_text(q) for q, a in conversations]
known_words = set(" ".join(known_questions).split())
max_seq_length = 120
MODEL_PATH = 'b1tler_gpt_model.pt'
TOKENIZER_PATH = 'b1tler_gpt_tokenizer.json'

d_model=192
num_heads=6
num_layers=4
d_ff=768

if os.path.exists(MODEL_PATH) and os.path.exists(TOKENIZER_PATH):
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = BPETokenizer()
    tokenizer.load(TOKENIZER_PATH)
    vocab_size = len(tokenizer.vocab)
    PAD_ID = tokenizer.token_to_id["<pad>"]
    model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, PAD_ID)
    model.load_state_dict(torch.load(MODEL_PATH))
    print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
else:
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è...")
    augmented_conversations = augment_data(conversations)
    print(f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(conversations)}, —Å—Ç–∞–ª–æ: {len(augmented_conversations)}")
    corpus = [q for q, a in augmented_conversations] + [a for q, a in augmented_conversations]
    tokenizer = BPETokenizer(vocab_size=150)
    tokenizer.train(corpus)
    vocab_size = len(tokenizer.vocab)
    PAD_ID = tokenizer.token_to_id["<pad>"]
    
    src_data_list, tgt_data_list, y_labels_list = [], [], []
    EOS_ID = tokenizer.token_to_id["<eos>"]
    SOS_ID = tokenizer.token_to_id["<sos>"]
    for q, a in augmented_conversations:
        src_tokens = tokenizer.encode(q)
        tgt_tokens = tokenizer.encode(a)
        src_data_list.append((src_tokens + [EOS_ID] + [PAD_ID] * max_seq_length)[:max_seq_length])
        tgt_data_list.append(([SOS_ID] + tgt_tokens + [PAD_ID] * max_seq_length)[:max_seq_length])
        y_labels_list.append((tgt_tokens + [EOS_ID] + [PAD_ID] * max_seq_length)[:max_seq_length])

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    src_data = torch.LongTensor(src_data_list).to(device)
    tgt_data = torch.LongTensor(tgt_data_list).to(device)
    y_labels = torch.LongTensor(y_labels_list).to(device)
    
    learning_rate=0.0001
    epochs=4000
    
    model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, PAD_ID).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)
    
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ PyTorch...")
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(src_data, tgt_data)
        loss = criterion(output.view(-1, vocab_size), y_labels.view(-1))
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 100 == 0:
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, –ü–æ—Ç–µ—Ä–∏: {loss.item():.4f}")
    
    torch.save(model.state_dict(), MODEL_PATH)
    tokenizer.save(TOKENIZER_PATH)
    print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

SOS_ID = tokenizer.token_to_id["<sos>"]
EOS_ID = tokenizer.token_to_id["<eos>"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
alias_memory = {}
context_memory = collections.defaultdict(list)

def _generate_single_response(clean_input):
    model.eval()
    src_tokens = tokenizer.encode(clean_input)
    src_tensor = torch.LongTensor([(src_tokens + [EOS_ID] + [PAD_ID] * max_seq_length)[:max_seq_length]]).to(device)
    output_ids = [SOS_ID]
    for _ in range(max_seq_length - 1):
        with torch.no_grad():
            tgt_tensor = torch.LongTensor([(output_ids + [PAD_ID] * max_seq_length)[:max_seq_length]]).to(device)
            output = model(src_tensor, tgt_tensor)
        
        last_logits = output[0, len(output_ids) - 1, :]
        last_word = tokenizer.decode([output_ids[-1]]).strip()
        
        if last_word in context_memory and random.random() < 0.7:
            new_word = random.choice(context_memory[last_word])
            response = tokenizer.decode(output_ids) + " " + new_word
            return response.replace("<sos>", "").strip()

        probs = torch.softmax(last_logits, dim=-1)
        next_word_id = torch.argmax(probs).item()
        
        if next_word_id == EOS_ID:
            break
        output_ids.append(next_word_id)
        
    raw_response = tokenizer.decode(output_ids)
    return raw_response.replace("<sos>", "").strip()

def chat(user_input):
    clean_input = clean_text(user_input)
    correction_threshold = 2
    original_words = clean_input.split()
    
    # –≠—Ç–∞–ø 1: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –∞–ª–∏–∞—Å–æ–≤
    normalized_input = " " + clean_input + " "
    for alias, known_phrase in sorted(alias_memory.items(), key=lambda item: len(item[0]), reverse=True):
        search_alias = " " + alias + " "
        if search_alias in normalized_input:
            print(f"(–ü–∞–º—è—Ç—å –∞–ª–∏–∞—Å–æ–≤: '{alias}' -> '{known_phrase}')")
            normalized_input = normalized_input.replace(search_alias, " " + known_phrase + " ")
    normalized_input = normalized_input.strip()

    # –≠—Ç–∞–ø 2: –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
    found_known_phrases = []
    remaining_input = " " + normalized_input + " "
    original_positions = {}
    for phrase in sorted(known_questions, key=len, reverse=True):
        search_phrase = " " + phrase + " "
        while search_phrase in remaining_input:
            pos = remaining_input.find(search_phrase)
            original_positions[phrase + str(pos)] = pos
            found_known_phrases.append(phrase)
            remaining_input = remaining_input.replace(search_phrase, " | ", 1)
    
    remaining_input = remaining_input.replace("|", " ").strip()

    # –≠—Ç–∞–ø 3: –ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–∞ –∏ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ
    if remaining_input:
        best_match = None
        min_dist = float('inf')
        all_known_items = known_questions + list(alias_memory.keys())
        for item in all_known_items:
            if remaining_input == item: continue
            dist = levenshtein_distance(remaining_input, item)
            if dist < min_dist:
                min_dist = dist
                best_match = item
        
        if min_dist <= correction_threshold:
            print(f"(–î—É–º–∞—é, '{remaining_input}' - —ç—Ç–æ –æ–ø–µ—á–∞—Ç–∫–∞ –≤ '{best_match}')")
            corrected_phrase = alias_memory.get(best_match, best_match)
            if corrected_phrase not in found_known_phrases:
                original_positions[corrected_phrase] = clean_input.find(remaining_input)
                found_known_phrases.append(corrected_phrase)
        else:
            if len(found_known_phrases) == 1:
                alias = remaining_input
                known_part = found_known_phrases[0]
                contains_known_words_in_alias = any(word in known_words for word in alias.split())
                if not contains_known_words_in_alias and alias not in known_questions and alias not in alias_memory:
                    alias_memory[alias] = known_part
                    print(f"(–ü–∞–º—è—Ç—å –∞–ª–∏–∞—Å–æ–≤: –∑–∞–ø–æ–º–Ω–∏–ª '{alias}' -> '{known_part}')")
            else:
                for i, word in enumerate(original_words):
                    if word not in known_words and i > 0:
                        prev_word = original_words[i-1]
                        if prev_word in known_words and word not in context_memory[prev_word]:
                            context_memory[prev_word].append(word)
                            print(f"(–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–∞–º—è—Ç—å: –ø–æ—Å–ª–µ '{prev_word}' –º–æ–∂–µ—Ç –∏–¥—Ç–∏ '{word}')")

    # –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if not found_known_phrases:
        if not clean_input:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–∞–∂–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å."
        best_match = None
        min_dist = float('inf')
        for question in known_questions:
            dist = levenshtein_distance(clean_input, question)
            if dist < min_dist:
                min_dist = dist
                best_match = question
        if min_dist <= correction_threshold:
            print(f"(–î—É–º–∞—é, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{best_match}')")
            found_known_phrases.append(best_match)
        else:
            return _generate_single_response(clean_input)
    
    # –≠—Ç–∞–ø 5: –°–±–æ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞
    sorted_phrases = sorted(found_known_phrases, key=lambda p: original_positions.get(p + str(normalized_input.find(p)), -1))
    responses = [_generate_single_response(phrase) for phrase in sorted_phrases]
    unique_responses = list(dict.fromkeys(responses))
    final_response = ", ".join(unique_responses)
    return final_response.capitalize() if final_response else "–Ø –Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è–ª, –º–æ–∂–µ—à—å –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å?"

print("\n–ú–æ–¥–µ–ª—å 4.0 —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ '–ø—Ä–∏–≤–µ—Ç –∫—Ç–æ —Ç–≤' –∏–ª–∏ '–∫–∞–∫ –¥–µ–ª–∞ –∞–ª–æ'.")
while True:
    user_message = input("–í—ã: ")
    if user_message.lower() == '–≤—ã—Ö–æ–¥':
        break
    response = chat(user_message)
    print(f"B1TLER-GPT: {response}")
